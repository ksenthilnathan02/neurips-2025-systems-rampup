# ğŸš€ NeurIPS 2025 â€” AI/ML Systems Ramp-Up

This repository is my structured, 48-hour learning ramp-up focused on the core AI/ML systems topics that are most relevant for NeurIPS 2025 recruiting and technical discussions.  
It contains **lightweight code demos + concise technical notes** covering the essentials of:

- âš¡ **LLM Inference Systems** (vLLM concepts, PagedAttention, batching)
- ğŸ§  **KV Cache Internals** + memory estimation script
- ğŸ¤– **Agentic Workflows** (LangGraph-style stateful pipelines)
- ğŸ–¼ï¸ **Multimodal Models** (VLM fundamentals: CLIP, SigLIP, LLaVA)
- ğŸ” **RAG Pipelines** (tiny example with FAISS + sentence-transformers)
- ğŸ“ˆ **Scaling Laws** (Chinchilla compute-optimal training basics)
- ğŸ§ª **Evaluations** (MMLU, MMMU, agent evals, tool correctness)

The goal is to demonstrate fluency across modern ML systems, prepare for conversations with researchers & applied scientists at NeurIPS, and show hands-on initiative through well-organized proof-of-work.

---
## ğŸ“ Repository Structure

```text
neurips-2025-systems-rampup/
â”‚
â”œâ”€â”€ agents/
â”‚   â””â”€â”€ research_agent.py                 # A simple LangGraph-style research pipeline
â”‚
â”œâ”€â”€ inference/
â”‚   â””â”€â”€ kv_cache_memory_estimator.py      # KV cache size calculator
â”‚
â”œâ”€â”€ rag/
â”‚   â””â”€â”€ tiny_rag_pipeline.py              # Mini RAG demo with FAISS
â”‚
â”œâ”€â”€ notes/                                # Concise notes for rapid ramp-up
â”‚   â”œâ”€â”€ llm_inference.md
â”‚   â”œâ”€â”€ kv_cache_notes.md
â”‚   â”œâ”€â”€ agent_systems.md
â”‚   â”œâ”€â”€ multimodal_basics.md
â”‚   â”œâ”€â”€ scaling_laws.md
â”‚   â””â”€â”€ evals_summary.md
â”‚
â””â”€â”€ README.md
```



---

## ğŸ¯ Purpose

This project is meant to:

- Build strong *systems intuition* across high-impact LLM topics  
- Enable confident conversations with NeurIPS recruiting teams  
- Serve as a quick reference for LLM inference, memory, agents, RAG, and multimodality  
- Showcase proactive learning through clean examples and organized notes  

Even small, clear demos can have a big impact when networking with researchers or hiring managers.

---

## ğŸ§© Topics Covered (Short Summaries)

### **1. LLM Inference (vLLM concepts)**
- KV cache is the main memory bottleneck  
- PagedAttention = virtualized KV storage for efficient batching  
- Continuous batching increases GPU utilization  
- Speculative decoding improves latency  

---

### **2. KV Cache Memory Estimation**
Includes a script that approximates memory usage:

memory â‰ˆ batch_size Ã— seq_len Ã— num_layers Ã— 2 Ã— hidden_size Ã— bytes


Useful for discussing throughput, batching, and long-context limits.

---

### **3. Agentic Systems**
- Graph-style control flow (LangGraph-inspired)
- Pipeline for summarization â†’ insight extraction â†’ question generation
- Focus on reliability, planning, and tool correctness

---

### **4. Multimodal Models**
- Vision encoder (CLIP/SigLIP) + language decoder  
- Image embeddings â†’ projected into LM token space  
- Training requires alignment + instruction tuning  

---

### **5. RAG Pipeline**
A minimal working RAG example using:

- Sentence-transformer embeddings  
- FAISS nearest-neighbor search  
- Simple retrieval wrapper  

---

### **6. Scaling Laws**
- Chinchilla: compute-optimal model size vs training tokens  
- Many large models are undertrained for their size  
- Motivates distillation + smaller optimized models  

---

### **7. Evaluations**
- MMLU, GSM8K, HELM for text  
- MMBench, MMMU for multimodal  
- Agent evals must test *multi-step reasoning* + *tool correctness*  

---

## ğŸ› ï¸ Setup (Optional)

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt   # if you add one
Or install packages manually (transformers, faiss-cpu, sentence-transformers).
```
---

ğŸ‘©â€ğŸ’» Author

Keerthana Senthilnathan
M.S. Data Science @ UC San Diego

---

ğŸŒŸ Notes

This repository represents a fast but focused systems ramp-up, not a deep research repo.
It is meant to show:

- Systems awareness

- Curiosity

- Ability to learn quickly

- Ability to organize technical information

- Genuine interest in ML systems

- Perfect for NeurIPS networking.


---
